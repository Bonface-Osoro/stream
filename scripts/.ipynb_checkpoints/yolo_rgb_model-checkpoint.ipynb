{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28268d29-8799-452c-870d-3412d39ad703",
   "metadata": {},
   "source": [
    "# Transfer Learning for Multiwavelength Drone Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c309f91-cd49-4236-88bf-abb73614528e",
   "metadata": {},
   "source": [
    "## Spring Season - May"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a117b6e-8181-4028-9cab-37335bf853f0",
   "metadata": {},
   "source": [
    "The objective of this project is to evaluate the performance of pretrained Convolution Neural Networks (CNNs) on different sets of data. Here we train the newly upgraded YOLOv8 on drone landmine images taken during Spring Season of May 2024. The model is trained on the data and then trained on the 2024 Winter Season. We then compare the performance of the model for images taken in the visible band and infrared band.  \n",
    "\n",
    "Here are the steps of producing the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beda0006-0a3b-42f3-b959-5a53b4be5829",
   "metadata": {},
   "source": [
    "First let's import the libraries that we are going to use for this tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2c023b5-21d0-42b5-b7a4-f36cf233905e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d632e8c5-d866-4384-9de4-3e65d242ca02",
   "metadata": {},
   "source": [
    "Next, we define the path location for our images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "da2f5e7d-e4b0-45f5-8f08-0fd9dc94d257",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_dir = \"../data/raw/may\"   \n",
    "image_dir = \"../data/raw/all_may_rgb\"  \n",
    "os.makedirs(image_dir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd6b83f-05d6-440c-bbb7-6a59fe604b26",
   "metadata": {},
   "source": [
    "### Pre-Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257e9a1c-4986-4d92-9743-f6852ee8e1e0",
   "metadata": {},
   "source": [
    "We want the images and the labels to have standard naming format such that the name tells you the period and the image type.\n",
    "\n",
    "For example; \"may_afternoon_0_lwir_89.xml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28632ef-7cfe-4618-92e3-e1226fb94072",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<b>!! Attention !!</b> DO NOT Run these cells twice. Even though, the code has been written to take care of that, it is recommended to avoid running it twice.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8140a81e-9c97-4c84-b891-d96989f7731d",
   "metadata": {},
   "source": [
    "First rename the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1c790498-f47f-4693-bf85-e5ab05265529",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"may_afternoon_0_\"\n",
    "\n",
    "for filename in os.listdir(image_dir):\n",
    "\n",
    "    if filename.endswith(\".jpg\") and not filename.startswith(prefix):\n",
    "        \n",
    "        old_path = os.path.join(image_dir, filename)\n",
    "        new_filename = prefix + filename\n",
    "        new_path = os.path.join(image_dir, new_filename)\n",
    "\n",
    "        os.rename(old_path, new_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaccf15-7839-442a-b103-974bfbce678b",
   "metadata": {},
   "source": [
    "Next rename the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "11ad2ed4-e762-49bd-93e9-0568a14d4538",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(image_dir):\n",
    "\n",
    "    if filename.endswith(\".xml\") and not filename.startswith(prefix):\n",
    "        \n",
    "        old_path = os.path.join(image_dir, filename)\n",
    "        new_filename = prefix + filename\n",
    "        new_path = os.path.join(image_dir, new_filename)\n",
    "\n",
    "        os.rename(old_path, new_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32add5a-d7d0-4d28-b5f6-ecdbd1098b71",
   "metadata": {},
   "source": [
    "Each of the objects in the images are labelled as either *'ap_metal', 'ap_plastic', 'at_metal'* or , *'at_plastic'*. Let us write a code that iterates through all the labels and extract these unique classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b089356a-cb75-49b5-9fc9-4e257fc3c7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes found: ['ap_metal', 'ap_plastic', 'at_metal', 'at_plastic']\n"
     ]
    }
   ],
   "source": [
    "unique_classes = set()\n",
    "\n",
    "for filename in os.listdir(image_dir):\n",
    "    \n",
    "    if filename.endswith(\".xml\"):\n",
    "        \n",
    "        filepath = os.path.join(image_dir, filename)\n",
    "        tree = ET.parse(filepath)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        # Iterate over each object tag\n",
    "        for obj in root.findall(\"object\"):\n",
    "            \n",
    "            class_name = obj.find(\"name\").text\n",
    "            unique_classes.add(class_name)\n",
    "\n",
    "# Convert to a sorted list\n",
    "class_list = sorted(list(unique_classes))\n",
    "\n",
    "print(\"Unique classes found:\", class_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22c1ede-07ff-4c9f-a2af-a3b7b81aeaeb",
   "metadata": {},
   "source": [
    "Next, we need to convert the labels into a format that is acceptable by YOLO. We achieve this by writing a function that accepts the \".xml\" annotation file, it extracts the image width and height, loops over each label to find the image class, check if its known against the class list from the previous code and then convert the class names into unique index as YOLO only recognizes IDs and not names.\n",
    "\n",
    "The function then extracts bounding box coordinates from the .xml file before converting it to YOLO format by normalizing them from 0 to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "72b1fa48-9a88-4a98-8eb0-4cc7a9a51ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_voc_to_yolo(xml_file):\n",
    "    \"\"\"\n",
    "    This function reads .xml annotation file, \n",
    "    extracts bounding boxes and class names \n",
    "    before converting them to YOLO format of \n",
    "    one string per object.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    xml_file : string\n",
    "        The path to a Pascal VOC-style XML \n",
    "        annotation label file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    w = int(root.find(\"size/width\").text)\n",
    "    h = int(root.find(\"size/height\").text)\n",
    "    \n",
    "    yolo_lines = []\n",
    "    for obj in root.findall(\"object\"):\n",
    "        \n",
    "        cls = obj.find(\"name\").text\n",
    "        if cls not in class_list:\n",
    "            \n",
    "            continue\n",
    "        cls_id = class_list.index(cls)\n",
    "        xmlbox = obj.find(\"bndbox\")\n",
    "        xmin = int(xmlbox.find(\"xmin\").text)\n",
    "        ymin = int(xmlbox.find(\"ymin\").text)\n",
    "        xmax = int(xmlbox.find(\"xmax\").text)\n",
    "        ymax = int(xmlbox.find(\"ymax\").text)\n",
    "\n",
    "        # Convert to YOLO format\n",
    "        x_center = ((xmin + xmax) / 2) / w\n",
    "        y_center = ((ymin + ymax) / 2) / h\n",
    "        bw = (xmax - xmin) / w\n",
    "        bh = (ymax - ymin) / h\n",
    "        yolo_lines.append(f\"{cls_id} {x_center} {y_center} {bw} {bh}\")\n",
    "        \n",
    "    return yolo_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901ab09f-e5b5-4762-8ce0-87ec40a1e239",
   "metadata": {},
   "source": [
    "We loop over the .xml files in label folder, convert the annotations from VOC format to YOLO using the our function and then save them as text files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0a165099-0d7b-4d00-93dc-7c5a06dba934",
   "metadata": {},
   "outputs": [],
   "source": [
    "for xml_file in os.listdir(image_dir):\n",
    "    \n",
    "    if not xml_file.endswith(\".xml\"):\n",
    "        \n",
    "        continue\n",
    "        \n",
    "    xml_path = os.path.join(image_dir, xml_file)\n",
    "    txt_path = os.path.join(image_dir, xml_file.replace(\".xml\", \".txt\"))\n",
    "    \n",
    "    yolo_data = convert_voc_to_yolo(xml_path)\n",
    "    with open(txt_path, \"w\") as f:\n",
    "        \n",
    "        f.write(\"\\n\".join(yolo_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c950db0-b813-4623-9ad6-3c02f75c5f3f",
   "metadata": {},
   "source": [
    "#### Preparing the data before training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4dff20-afa8-48c1-97e6-68717d50ed0b",
   "metadata": {},
   "source": [
    "Now that we have processed the data, our task is now to split the data into \"train\", \"validation\" and \"test\".\n",
    "\n",
    "We train the YOLO model on 75% of the data, we then validate it on 20% of the data and then test it on the remaining 5%. However, before splitting the data into three portions, we need to shuffle and randomize them as shown in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ae3c9701-1ac1-4c4f-8b91-d6e64110dd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_base = \"../results/rgb/dataset\"\n",
    "train_ratio, val_ratio, test_ratio = 0.75, 0.2, 0.5\n",
    "\n",
    "#Shuffle the original images\n",
    "images = [f for f in os.listdir(image_dir) if f.endswith((\".jpg\", \".png\"))]\n",
    "random.shuffle(images)\n",
    "\n",
    "# Compute split indices\n",
    "total = len(images)\n",
    "train_end = int(total * train_ratio)\n",
    "val_end = train_end + int(total * val_ratio)\n",
    "\n",
    "# Split image filenames\n",
    "split_data = {\"train\": images[:train_end], \"val\": images[train_end:val_end],\n",
    "    \"test\": images[val_end:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6375a655-bf98-404f-92b2-87a209b0144c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4329"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913944a2-fc2e-49cc-aa2b-dd280fd1bf97",
   "metadata": {},
   "source": [
    "We then dynamically create the \"images\" and \"labels\" folders to store the training, validation and test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c63922e-d906-49c0-9712-73cecc57292d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b>Note:</b> This cell will likely return some warning of missing labels for some images. There is no need to worry about this since some images did not have the target objects.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5cb05014-e6cf-47b1-a52b-734bffb17cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Label not found for image: may_noon_40_3_lwir_103(1).jpg\n",
      "‚ö†Ô∏è Label not found for image: may_noon_40_3_lwir_101(1).jpg\n",
      "‚ö†Ô∏è Label not found for image: may_noon_40_3_lwir_104(1).jpg\n",
      "‚ö†Ô∏è Label not found for image: may_afternoon_20_0_lwir_85(1).jpg\n",
      "‚ö†Ô∏è Label not found for image: may_noon_40_3_lwir_1(1).jpg\n",
      "‚ö†Ô∏è Label not found for image: may_noon_40_2_lwir_9(1).jpg\n",
      "‚ö†Ô∏è Label not found for image: may_noon_40_3_lwir_102(1).jpg\n",
      "‚ö†Ô∏è Label not found for image: may_noon_40_2_lwir_72(1).jpg\n",
      "‚ö†Ô∏è Label not found for image: may_noon_40_3_lwir_100(1).jpg\n",
      "‚ö†Ô∏è Label not found for image: may_noon_40_3_lwir_10(1).jpg\n"
     ]
    }
   ],
   "source": [
    "# Create folder structure and copy files\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    img_out_dir = os.path.join(output_base, \"images\", split)\n",
    "    lbl_out_dir = os.path.join(output_base, \"labels\", split)\n",
    "    os.makedirs(img_out_dir, exist_ok=True)\n",
    "    os.makedirs(lbl_out_dir, exist_ok=True)\n",
    "\n",
    "    for img_file in split_data[split]:\n",
    "        # Copy image\n",
    "        shutil.copy(os.path.join(image_dir, img_file), os.path.join(img_out_dir, img_file))\n",
    "\n",
    "        # Copy corresponding label\n",
    "        txt_file = os.path.splitext(img_file)[0] + \".txt\"\n",
    "        src_lbl = os.path.join(image_dir, txt_file)\n",
    "        if os.path.exists(src_lbl):\n",
    "            shutil.copy(src_lbl, os.path.join(lbl_out_dir, txt_file))\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Label not found for image: {img_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e47ff8-bb1d-4d65-9348-0c6bacddd2cd",
   "metadata": {},
   "source": [
    "Next we need to create a .yaml file that tells YOLOv8 model where our dataset is and the classes that we are using. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0aa655-08ae-4ed6-89f6-4ba47f253c89",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b>Note:</b> A yaml file is a plain-text configuration file format commonly used to store structured data into human-readable way especially for machine learning models and also describing metadata.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1e64e272-da8c-4493-a2f9-bebe04f221f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"path\":output_base, \n",
    "    \"train\": os.path.join(\"/Users/bosoro/Documents/GitHub/flight/results/rgb/dataset/images/train\"),\n",
    "    \"val\": os.path.join(\"/Users/bosoro/Documents/GitHub/flight/results/rgb/dataset/images/val\"),\n",
    "    \"test\": os.path.join(\"/Users/bosoro/Documents/GitHub/flight/results/rgb/dataset/images/test\"),\n",
    "    \"nc\": len(class_list),\n",
    "    \"names\": class_list,\n",
    "}\n",
    "\n",
    "yaml_path = os.path.join(output_base, \"data.yaml\")\n",
    "with open(yaml_path, \"w\") as f:\n",
    "    \n",
    "    yaml.dump(data, f, default_flow_style = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b2b549-c3a2-4423-a2f6-803829fef5b9",
   "metadata": {},
   "source": [
    "Now we load our YOLOv8 using the ultralytics library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e2d1be18-0146-48d3-ae1f-e15c5957ff49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolov8n.pt\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541edafb-cf86-49f4-9a97-ee5671af04ab",
   "metadata": {},
   "source": [
    "We then use the model to train our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e11e20-02f6-43b2-b6fc-18cfa91c11c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.198 available üòÉ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.186 üöÄ Python-3.11.11 torch-2.8.0 CPU (Apple M1 Pro)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=../results/rgb/dataset/data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train2, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=50, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/train2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=4\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    752092  ultralytics.nn.modules.head.Detect           [4, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,011,628 parameters, 3,011,612 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 196.8¬±117.3 MB/s, size: 188.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/bosoro/Documents/GitHub/flight/results/rgb/dataset/labels/train... 3240 images, 6 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3246/3246 1786.8it/s 1.8s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/bosoro/Documents/GitHub/flight/results/rgb/dataset/labels/train.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 337.5¬±62.1 MB/s, size: 170.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/bosoro/Documents/GitHub/flight/results/rgb/dataset/labels/val... 862 images, 3 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 865/865 1672.7it/s 0.5s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Users/bosoro/Documents/GitHub/flight/results/rgb/dataset/labels/val.cache\n",
      "Plotting labels to runs/detect/train2/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train2\u001b[0m\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      1/100         0G      1.438      1.794     0.9146        217        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 203/203 0.18it/s 18:42\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 28/28 0.25it/s 1:51s\n",
      "                   all        865       8463      0.493      0.446      0.462      0.254\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      2/100         0G      1.352      1.069     0.8972        227        640:  45% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 92/203 0.18it/s 8:51"
     ]
    }
   ],
   "source": [
    "yaml_file = os.path.join(output_base, \"data.yaml\")\n",
    "model.train(data = yaml_file, epochs = 100, patience = 50, imgsz = 640, batch = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749129d8-69f4-40c8-9a16-ec26149e91f3",
   "metadata": {},
   "source": [
    "### Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a0f5bd-71c4-4ffe-b328-8a1d3cebbdb3",
   "metadata": {},
   "source": [
    "Now that we have trained our model, we can test it on the 5% images that we set aside earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bbd339-4ee6-4b9d-b374-55b928ee616e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b>Note:</b> YOLOv8 automatically saves the model on training. The saved model can be found in this path where the training script is located. *runs/detect/train/exp*/weights/*\n",
    "\n",
    "The model is automatically named as *\"best.pt\"*\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed16e81e-1592-4c7b-bf1c-4695a7ce333b",
   "metadata": {},
   "source": [
    "We first define the location of the saved model that we have just trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a95566c1-bf4b-48d7-a74c-a84eea9bb0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(\"/Users/bosoro/Documents/GitHub/flight/scripts/runs/detect/train/weights\", \"best.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3b4c84-1bfc-4d28-8a38-08d27e74bb36",
   "metadata": {},
   "source": [
    "We also define the path of the test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "abccce1e-3660-4311-939c-740e76662830",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = os.path.join(\"../results/may/dataset/images/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7fb695-ed46-4376-ad6a-642795bcda1b",
   "metadata": {},
   "source": [
    "We now load the saved model in notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b961a6c-59c7-4a61-b2b9-b8568be3dca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "may_model = YOLO(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95342e57-accf-4704-b09f-33ff598d2f45",
   "metadata": {},
   "source": [
    "And make predictions on the test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438e0cca-c891-4d8f-bf58-3f93337a2c49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = may_model.predict(source = test_images, save = True, imgsz = 640)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d606a5-17d7-44c8-99b9-daa84009443a",
   "metadata": {},
   "source": [
    "Now we quantitavely evaluate the model on the test data to understand its performance on the data that it has not seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4f57c73d-4589-451a-bec7-db8dc8d2b801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.186 üöÄ Python-3.11.11 torch-2.8.0 CPU (Apple M1 Pro)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.2¬±0.1 ms, read: 386.6¬±64.7 MB/s, size: 186.7 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/bosoro/Documents/GitHub/flight/results/may/dataset/labels/test.cache... 42 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 42/42 101650.8it/s 0.0s0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 0.58it/s 5.1s\n",
      "                   all         42        360      0.872      0.902      0.921      0.547\n",
      "              ap_metal         22         38      0.799      0.868      0.875      0.361\n",
      "            ap_plastic         26         52      0.749      0.769      0.835      0.431\n",
      "              at_metal         30         45      0.961      0.978      0.979      0.662\n",
      "            at_plastic         41        225      0.977      0.991      0.994      0.735\n",
      "Speed: 0.8ms preprocess, 105.6ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "metrics = may_model.val(data = yaml_file, split = \"test\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f72543e-3a12-4b82-b42f-eca3bde2e65b",
   "metadata": {},
   "source": [
    "Next we save the performance metrics for the model for future comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a25f995c-5dda-47ac-9ec6-c16c064bd80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = may_model.names\n",
    "rows = []\n",
    "for i, name in class_names.items():\n",
    "    \n",
    "    p, r, ap50, ap = metrics.box.class_result(i)\n",
    "    rows.append({\"class_id\": i, \"class_name\": name, \"precision\": p,\n",
    "        \"recall\": r, \"ap50\": ap50, \"ap50_95\": ap,\n",
    "        \"season\": \"spring\", \"band\": \"optical\"})\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "RESULTS = \"../results\"\n",
    "output_file = os.path.join(RESULTS, \"may_images_metrics.csv\")\n",
    "df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5689f251-9fd5-415f-82c0-18b69e17196f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
